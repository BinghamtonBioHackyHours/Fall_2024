---
format: typst
---

# What are multilevel (mixed effects, hierarchical, etc) models

There are many, often conflicting, definitions for these terms. For the time being let's think about multilevel models as a model where the parameters, at one level of the model, are themselves modeled in another level.

Just a reminder, of our **boring** single level model:

$$
\begin{align*}
y_i &\sim Normal(\mu_i, \sigma)\\
\mu_i &= \alpha + \beta x_i
\end{align*}
$$

Where $\alpha$ is the intercept. We can interpret this as the value of $\mu$ when $x=0$. Since this is often nonsense, biologically, we can center the predictor by subtracting the mean, this will allow us to interpret the intercept as the expected value of $y$ at the mean of the predictor.

$\beta$ then is the slope, i.e. for a one unit increase in $x$ there is a $\beta$ increase in $y$.

In the multilevel model, $\alpha$ and/or $\beta$ are now going to be modeled at a second level in the model. In the case where we model the intercept (i.e. a **varying intercepts** model), we would have something like this:

$$
\begin{align*}
y_{i} &\sim Normal(\mu_i, \sigma)\\
\mu_i &= \alpha_{j[i]} + \beta x_i\\
\alpha_j &\sim Normal(\overline{\alpha}, \sigma_{\alpha})
\end{align*}
$$

# Constructing a multilevel model, Always be simulatin'.

Let's begin, as usual, with Allie working her way up a stream in search of *Anolis aquaticus*.



As Allie bravely works upstream, she is thinking about her past research on this stream and thinks fondly of her past modeling experiences. She knows that anoles on this stream will dive to escape predators and will spend a certain amount of time underwater that follows the following model:

$$
\begin{align*}
time_i &\sim Normal(\mu_i, .5)\\
\mu_i &= 3 + .5\times size_i
\end{align*}
$$

Let's simulate from and plot this model.

```{r}

n <- 100
a <- 3
b <- .5
sigma <- .5
size <- rnorm(n)

mu <- a + b*size

time <- rnorm(n, mu, sigma)

plot(time ~ size, pch = 20)
abline(a = 3, b = .5, col = "red", lwd = 2)


```

Allie is interested if these findings are unique to this stream, or if it can be generalized to the entire population of anoles. So she collects data from 10 different streams and measures how long each anole stays submerged. Let's read in the data and check it out. Let's do a quick little plot to see if diving looks correlated with size. (n.b. size is scaled to have a mean of 0 and a standard deviation of 1)

```{r warning=FALSE, message=FALSE}
library(here)
library(lme4)
library(arm)
df <- read.csv(here("data/week_10/anoles.csv"))
df
plot(df$time ~ df$size, pch = 20)
```

# Complete Pooling, No Pooling, Partial Pooling

## Complete Pooling

As the name suggests, we will completely pool the data together, just like we have been doing so far. Here's the model:

$$
\begin{align*}
time_i &\sim Normal(\mu_i, \sigma)\\
\mu_i &= \alpha + \beta size_i
\end{align*}
$$

Let's fit it.

```{r}

mod_pool <- lm(time ~ 1 + size, data = df)
display(mod_pool)
summary(mod_pool)


```

```{r}
plot(time ~ size, data = df, col = "lavenderblush3", pch = 20)
abline(a = coef(mod_pool)[1], b = coef(mod_pool)[2], col = "darkslateblue")
hist(df$time)
```

So the complete pooling robot is fairly confident that smaller lizards dive longer. Does Allie need to update her thinking on size and predator avoidance? Was the original research an artifact of sampling? Maybe there is an effect of stream and she just happened to sample an outlier stream?

Let's try adding stream as a predictor.

## No Pooling

In this case, we will add stream directly into the model, so there will be a separate intercept for each stream. Importantly, there is no information shared between streams in the model:

$$
\begin{align*}
time_i &\sim Normal(\mu_i, \sigma)\\
\mu_i &= \alpha_{stream[i]} + \beta size_i
\end{align*}
$$

```{r}

df$stream <- factor(df$stream)
display(mod_no_pool <- lm(time ~ -1 + stream + size, data = df))

```

The no pooling robot seems fairly confident that there is a positive relationship between size and diving. Maybe Allie's research was correct on the first stream?

Let's try the partial pooling approach.

# Partial Pooling

Like the name suggests, this approach (the multilevel approach), allows us to have separate groups, but they share information. We do this by forcing the intercepts to come from a common distribution. Here's the model:

$$
\begin{align*}
time_i &\sim Normal(\mu_i, \sigma)\\
\mu_i &= \alpha_{stream[i]} + \beta size_i\\
\alpha_{stream[i]} &\sim Normal(\overline{\alpha}, \sigma_{\alpha})
\end{align*}
$$

Let's look at this model.

```{r}

mod_p_pool <- lmer(time ~ 1 + size + (1|stream), data = df)
display(mod_p_pool)
fixef(mod_p_pool)
ranef(mod_p_pool)

```

Now we've gone to the robot being unsure about what is going on. Let's do some plotting to think through it.

```{r}

library(tidyverse)

base <- df %>% 
  ggplot(aes(x = size, y = time)) +
  geom_point() +
  facet_wrap(stream ~ .)
  
base

```

Let's add some lines that represent our no-pooling and partial-pooling models. We will also add a vertical line at 0 for the mean size and a horizontal line for the mean of the stream distribution (since you know the malevolent god in charge of this data, you know that the true mean of the streams is 5!)

```{r}

base <- base +
  geom_hline(yintercept = 5, color = "grey") +
  geom_vline(xintercept = 0, color = "grey")
base
```

Well, there are 3 streams where Allie was only able to sample 1 anole, What do we think about that?

### Plot the mean fit on each facet

We need to extract the intercepts and slopes for each model, and put them in a data frame with streams to add them to ggplot.

```{r message=FALSE, warning=FALSE}

fit_no_pool <- data.frame(stream = factor(1:10), b = coef(mod_no_pool)[11], a = coef(mod_no_pool)[1:10])
fit_p_pool <- data.frame(stream = factor(1:10), b = fixef(mod_p_pool)[2], fixef(mod_p_pool)[1] + ranef(mod_p_pool)$stream)
colnames(fit_p_pool) <- c("stream", "b", "a")

base <- base +
  geom_abline(data = fit_no_pool, aes(slope = b, intercept = a))

base
```

This looks pretty good! Actually, it looks almost too good. Check out the three streams that have only one data point. The model has fit those points exactly. If you aren't paranoid already, this should spark any latent paranoia lurking in your mind.

Let's add the mean fit of the partially pooled model to see what's going on.

```{r}

base +
  geom_abline(data = fit_p_pool, aes(slope = b, intercept = a), col = "blue")
```

So what is going on here?  
  
   
Shrinkage to the mean!  


If you look at the intercepts of the blue lines (where the blue line crosses the 0 vertical line) they are shrunk toward the true population mean. This means that they are less likely to over fit the data. In the no-pooling model, the streams with more points dominate the calculation of the slope. For the streams with only one point, there is no information about the slope, so they meekly accept the slope calculated for the better observed streams, this means that the intercept is then fit so that the regression line goes through the single point. This causes massive over fitting.

In the case of the partially pooled model, the streams come from a distribution with mean $\overline{\alpha}$ and a standard deviation of $\sigma_{\alpha}$. This does two things. First, the intercept of each stream is now a compromise between the data point of that stream and the distribution of streams. The model will penalize stream values that are two far away from the population mean (of streams). Second, we can now use the information in those individual points to estimate the slope. They will still carry less weight than the better observed streams, but we are not throwing away that information. All of this gives us a more conservative model. To get more resolution, Allie will need to do more sampling.

Allie has also learned that there is considerable variability in behavior between streams. She can start thinking about what would cause this variation. Importantly, the multilevel model let's her make inferences about the differences between streams where single level models cannot!!

Let's try to simulate from these things to get a better understanding!
